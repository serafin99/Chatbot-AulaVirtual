{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMXLRjnmdjdv",
        "outputId": "b866b426-6711-4ec1-ac79-23730317cd67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMa-Ah26dtWF"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Configurar la API Key (reemplázala con tu clave real)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFu4gOukejpA"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Configurar la API Key (reemplázala con tu clave real)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "F8i4HlBaeome",
        "outputId": "92118c52-e5e2-474f-d913-16fe902c1c3a"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-28b6c7fb815e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Prueba la función\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hola, ¿cómo estás?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-28b6c7fb815e>\u001b[0m in \u001b[0;36mchat_with_gpt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Cambia \"gpt-4\" por \"gpt-4-turbo\" o \"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "def chat_with_gpt(prompt):\n",
        "    client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Cambia \"gpt-4\" por \"gpt-4-turbo\" o \"gpt-3.5-turbo\"\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Prueba la función\n",
        "user_input = \"Hola, ¿cómo estás?\"\n",
        "response = chat_with_gpt(user_input)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMSHcBYpfa0r",
        "outputId": "3f756759-b4cb-4a68-cbb0-0803c07ee0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg99GOddhvXZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omdYzs2KhwFB"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\"\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza esto con tu clave de API real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHwQaMQeh5En"
      },
      "outputs": [],
      "source": [
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6LieHz7h8hk"
      },
      "outputs": [],
      "source": [
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwWTAo2EiBQX",
        "outputId": "e8b2336e-2c1e-479c-cc53-cc9f424ca31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Soy un chatbot creado con la API de Gemini.\n",
            "Tú: hola\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n",
            "Tú: adiós\n",
            "Chatbot: ¡Adiós! Fue un placer hablar contigo.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\"\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza esto con tu clave de API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"¡Hola! Soy un chatbot creado con la API de Gemini.\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(usuario_input)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "IQCh7n3slfo1",
        "outputId": "f139d9b6-e0d4-4b03-9b9c-95e19290796f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Soy un chatbot creado con la API de Gemini.\n",
            "Tú: candidates\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n",
            "Tú: hola\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n",
            "Tú: chau\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n",
            "Tú: ¿Cómo estás?\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c106a060b4f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0musuario_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0musuario_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adiós\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasta luego\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\"\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza esto con tu clave de API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"¡Hola! Soy un chatbot creado con la API de Gemini.\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(usuario_input)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "rpBEHWmql7Y5",
        "outputId": "093bd9f9-c6a4-4ef6-99e2-6e4b0db33b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Soy un chatbot creado con la API de Gemini.\n",
            "Tú: hola\n",
            "{'error': {'code': 404, 'message': 'models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\n",
            "Chatbot: Lo siento, no pude procesar la respuesta.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c60704935e6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0musuario_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0musuario_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adiós\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasta luego\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent\"\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza esto con tu clave de API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    print(respuesta_json)  # Imprime la respuesta JSON de la API\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"¡Hola! Soy un chatbot creado con la API de Gemini.\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(usuario_input)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BfM0_yPmMOS",
        "outputId": "05e5dee8-0826-4d27-e037-ea11f8cdbc02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"models\": [\n",
            "    {\n",
            "      \"name\": \"models/chat-bison-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"PaLM 2 Chat (Legacy)\",\n",
            "      \"description\": \"A legacy text-only model optimized for chat conversations\",\n",
            "      \"inputTokenLimit\": 4096,\n",
            "      \"outputTokenLimit\": 1024,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateMessage\",\n",
            "        \"countMessageTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.25,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/text-bison-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"PaLM 2 (Legacy)\",\n",
            "      \"description\": \"A legacy model that understands text and generates text as an output\",\n",
            "      \"inputTokenLimit\": 8196,\n",
            "      \"outputTokenLimit\": 1024,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateText\",\n",
            "        \"countTextTokens\",\n",
            "        \"createTunedTextModel\"\n",
            "      ],\n",
            "      \"temperature\": 0.7,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/embedding-gecko-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Embedding Gecko\",\n",
            "      \"description\": \"Obtain a distributed representation of a text.\",\n",
            "      \"inputTokenLimit\": 1024,\n",
            "      \"outputTokenLimit\": 1,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"embedText\",\n",
            "        \"countTextTokens\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.0-pro-vision-latest\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.0 Pro Vision\",\n",
            "      \"description\": \"The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\",\n",
            "      \"inputTokenLimit\": 12288,\n",
            "      \"outputTokenLimit\": 4096,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.4,\n",
            "      \"topP\": 1,\n",
            "      \"topK\": 32\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-pro-vision\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.0 Pro Vision\",\n",
            "      \"description\": \"The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\",\n",
            "      \"inputTokenLimit\": 12288,\n",
            "      \"outputTokenLimit\": 4096,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.4,\n",
            "      \"topP\": 1,\n",
            "      \"topK\": 32\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-pro-latest\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Pro Latest\",\n",
            "      \"description\": \"Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\",\n",
            "      \"inputTokenLimit\": 2000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-pro-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Pro 001\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\",\n",
            "      \"inputTokenLimit\": 2000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-pro-002\",\n",
            "      \"version\": \"002\",\n",
            "      \"displayName\": \"Gemini 1.5 Pro 002\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\",\n",
            "      \"inputTokenLimit\": 2000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-pro\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Pro\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\",\n",
            "      \"inputTokenLimit\": 2000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-latest\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash Latest\",\n",
            "      \"description\": \"Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash 001\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-001-tuning\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash 001 Tuning\",\n",
            "      \"description\": \"Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\",\n",
            "      \"inputTokenLimit\": 16384,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createTunedModel\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash\",\n",
            "      \"description\": \"Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-002\",\n",
            "      \"version\": \"002\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash 002\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"createCachedContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-8b\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash-8B\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"createCachedContent\",\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-8b-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash-8B 001\",\n",
            "      \"description\": \"Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"createCachedContent\",\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-8b-latest\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash-8B Latest\",\n",
            "      \"description\": \"Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"createCachedContent\",\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-8b-exp-0827\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash 8B Experimental 0827\",\n",
            "      \"description\": \"Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-1.5-flash-8b-exp-0924\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemini 1.5 Flash 8B Experimental 0924\",\n",
            "      \"description\": \"Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\",\n",
            "      \"inputTokenLimit\": 1000000,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.5-pro-exp-03-25\",\n",
            "      \"version\": \"2.5-exp-03-25\",\n",
            "      \"displayName\": \"Gemini 2.5 Pro Experimental 03-25\",\n",
            "      \"description\": \"Experimental release (March 25th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-exp\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash Experimental\",\n",
            "      \"description\": \"Gemini 2.0 Flash Experimental\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"bidiGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash\",\n",
            "      \"description\": \"Gemini 2.0 Flash\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-001\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash 001\",\n",
            "      \"description\": \"Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-exp-image-generation\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash (Image Generation) Experimental\",\n",
            "      \"description\": \"Gemini 2.0 Flash (Image Generation) Experimental\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\",\n",
            "        \"bidiGenerateContent\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-lite-001\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash-Lite 001\",\n",
            "      \"description\": \"Stable version of Gemini 2.0 Flash Lite\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-lite\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash-Lite\",\n",
            "      \"description\": \"Gemini 2.0 Flash-Lite\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-lite-preview-02-05\",\n",
            "      \"version\": \"preview-02-05\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash-Lite Preview 02-05\",\n",
            "      \"description\": \"Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-lite-preview\",\n",
            "      \"version\": \"preview-02-05\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash-Lite Preview\",\n",
            "      \"description\": \"Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 40,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-pro-exp\",\n",
            "      \"version\": \"2.5-exp-03-25\",\n",
            "      \"displayName\": \"Gemini 2.0 Pro Experimental\",\n",
            "      \"description\": \"Experimental release (March 25th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-pro-exp-02-05\",\n",
            "      \"version\": \"2.5-exp-03-25\",\n",
            "      \"displayName\": \"Gemini 2.0 Pro Experimental 02-05\",\n",
            "      \"description\": \"Experimental release (March 25th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-exp-1206\",\n",
            "      \"version\": \"2.5-exp-03-25\",\n",
            "      \"displayName\": \"Gemini Experimental 1206\",\n",
            "      \"description\": \"Experimental release (March 25th, 2025) of Gemini 2.5 Pro\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-thinking-exp-01-21\",\n",
            "      \"version\": \"2.0-exp-01-21\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash Thinking Experimental 01-21\",\n",
            "      \"description\": \"Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.7,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-thinking-exp\",\n",
            "      \"version\": \"2.0-exp-01-21\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash Thinking Experimental 01-21\",\n",
            "      \"description\": \"Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.7,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-2.0-flash-thinking-exp-1219\",\n",
            "      \"version\": \"2.0\",\n",
            "      \"displayName\": \"Gemini 2.0 Flash Thinking Experimental\",\n",
            "      \"description\": \"Gemini 2.0 Flash Thinking Experimental\",\n",
            "      \"inputTokenLimit\": 1048576,\n",
            "      \"outputTokenLimit\": 65536,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 0.7,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/learnlm-1.5-pro-experimental\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"LearnLM 1.5 Pro Experimental\",\n",
            "      \"description\": \"Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\",\n",
            "      \"inputTokenLimit\": 32767,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64,\n",
            "      \"maxTemperature\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemma-3-27b-it\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Gemma 3 27B\",\n",
            "      \"inputTokenLimit\": 131072,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateContent\",\n",
            "        \"countTokens\"\n",
            "      ],\n",
            "      \"temperature\": 1,\n",
            "      \"topP\": 0.95,\n",
            "      \"topK\": 64\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/embedding-001\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Embedding 001\",\n",
            "      \"description\": \"Obtain a distributed representation of a text.\",\n",
            "      \"inputTokenLimit\": 2048,\n",
            "      \"outputTokenLimit\": 1,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"embedContent\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/text-embedding-004\",\n",
            "      \"version\": \"004\",\n",
            "      \"displayName\": \"Text Embedding 004\",\n",
            "      \"description\": \"Obtain a distributed representation of a text.\",\n",
            "      \"inputTokenLimit\": 2048,\n",
            "      \"outputTokenLimit\": 1,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"embedContent\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-embedding-exp-03-07\",\n",
            "      \"version\": \"exp-03-07\",\n",
            "      \"displayName\": \"Gemini Embedding Experimental 03-07\",\n",
            "      \"description\": \"Obtain a distributed representation of a text.\",\n",
            "      \"inputTokenLimit\": 8192,\n",
            "      \"outputTokenLimit\": 1,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"embedContent\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/gemini-embedding-exp\",\n",
            "      \"version\": \"exp-03-07\",\n",
            "      \"displayName\": \"Gemini Embedding Experimental\",\n",
            "      \"description\": \"Obtain a distributed representation of a text.\",\n",
            "      \"inputTokenLimit\": 8192,\n",
            "      \"outputTokenLimit\": 1,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"embedContent\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/aqa\",\n",
            "      \"version\": \"001\",\n",
            "      \"displayName\": \"Model that performs Attributed Question Answering.\",\n",
            "      \"description\": \"Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\",\n",
            "      \"inputTokenLimit\": 7168,\n",
            "      \"outputTokenLimit\": 1024,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"generateAnswer\"\n",
            "      ],\n",
            "      \"temperature\": 0.2,\n",
            "      \"topP\": 1,\n",
            "      \"topK\": 40\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"models/imagen-3.0-generate-002\",\n",
            "      \"version\": \"002\",\n",
            "      \"displayName\": \"Imagen 3.0 002 model\",\n",
            "      \"description\": \"Vertex served Imagen 3.0 002 model\",\n",
            "      \"inputTokenLimit\": 480,\n",
            "      \"outputTokenLimit\": 8192,\n",
            "      \"supportedGenerationMethods\": [\n",
            "        \"predict\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza con tu clave API real\n",
        "\n",
        "def listar_modelos():\n",
        "    url = \"https://generativelanguage.googleapis.com/v1beta/models\"\n",
        "    headers = {\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "modelos = listar_modelos()\n",
        "print(json.dumps(modelos, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "O7eV1Ou7mYkp",
        "outputId": "0257c0a5-a8ae-4fb0-92c9-a82be5d69e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Soy un chatbot creado con la API de Gemini.\n",
            "Tú: hola\n",
            "Chatbot: Hola! ¿Cómo estás?\n",
            "\n",
            "Tú: todo bien\n",
            "Chatbot: ¡Todo bien!  ¿Y tú?\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8fd160554c8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0musuario_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0musuario_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adiós\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasta luego\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent\"  # Cambiado a gemini-1.5-pro-latest\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza esto con tu clave de API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"¡Hola! Soy un chatbot creado con la API de Gemini.\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(usuario_input)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "Ufv9enY3nqXr",
        "outputId": "31c24b57-ef86-419d-f7af-5809d8962431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, soy Dudacero.IA ¿En qué puedo ayudarte hoy? Puedes preguntarme sobre fechas, textos o criterios de evaluación.\n",
            "Tú: Resume el texto vijilar y castigar en tres puntos clave.\n",
            "Chatbot: Los tres puntos clave de *Vigilar y Castigar* de Michel Foucault son:\n",
            "\n",
            "1. **El cambio del castigo del cuerpo al alma:** Foucault argumenta que el castigo ha evolucionado desde un espectáculo público de tortura y ejecución del cuerpo del criminal, hacia métodos más sutiles de control social enfocados en la vigilancia, disciplina y corrección del \"alma\" o la mente del individuo.  Este cambio no representa una humanización del sistema penal, sino una transformación en las técnicas de poder.\n",
            "\n",
            "2. **El poder como omnipresente y productivo:** En lugar de ver el poder como algo represivo y localizado en el Estado, Foucault lo describe como una red difusa y capilar que atraviesa toda la sociedad.  El poder no sólo reprime, sino que también produce sujetos dóciles a través de instituciones como la escuela, el hospital y la prisión, que operan bajo el modelo del \"panóptico\".\n",
            "\n",
            "3. **El panóptico como modelo de control social:** El panóptico, un diseño arquitectónico de prisión ideado por Jeremy Bentham, funciona como metáfora del funcionamiento del poder en la sociedad moderna. La posibilidad constante de ser vigilado, aunque no se sepa si se está siendo observado en un momento dado, internaliza la disciplina y la autovigilancia en los individuos.  Esta lógica del panóptico se extiende más allá de la prisión, influyendo en diversas instituciones y moldeando la conducta de los sujetos en la sociedad.\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4918e0f26211>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0musuario_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0musuario_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"salir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adiós\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hasta luego\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent\"  # Reemplaza con la URL de la API de Gemini\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza con tu clave API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"Hola, soy Dudacero.IA ¿En qué puedo ayudarte hoy? Puedes preguntarme sobre fechas, textos o criterios de evaluación.\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    # Funciones de agente con prompts predefinidos\n",
        "    if \"resumen\" in usuario_input.lower():\n",
        "        texto_a_resumir = usuario_input.replace(\"resumen \", \"\")\n",
        "        prompt = f\"Resume el texto {texto_a_resumir} en tres puntos clave.\"\n",
        "    elif \"fecha límite\" in usuario_input.lower():\n",
        "        actividad = usuario_input.replace(\"¿cuál es la fecha límite para la entrega de \", \"\")\n",
        "        prompt = f\"¿Cuál es la fecha límite para la entrega de {actividad}?\"\n",
        "    elif \"criterios de evaluación\" in usuario_input.lower():\n",
        "        actividad = usuario_input.replace(\"explica los criterios de evaluación para \", \"\")\n",
        "        prompt = f\"Explica los criterios de evaluación para {actividad}.\"\n",
        "    elif \"temas de la próxima clase\" in usuario_input.lower():\n",
        "        prompt = \"¿Qué temas se verán en la próxima clase?\"\n",
        "    else:\n",
        "        prompt = usuario_input\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(prompt)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKl0KTYwo5RM",
        "outputId": "f62d57cd-86c8-4e67-9032-644caab49100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, soy Dudacero.IA ¿En qué puedo ayudarte hoy?\n",
            "1. Resumir texto\n",
            "2. Cronograma y fechas de evaluación\n",
            "3. Temas de la próxima clase\n",
            "Chatbot: No tengo acceso a tu programa de estudios ni a tu calendario de clases. Para saber qué temas se verán en tu próxima clase, te recomiendo:\n",
            "\n",
            "* **Revisar tu programa de estudios o sílabo:**  Ahí suele detallarse el contenido de cada sesión.\n",
            "* **Revisar la plataforma de aprendizaje de tu clase (Moodle, Blackboard, Canvas, etc.):**  Los profesores suelen publicar la información de las próximas clases, incluyendo los temas a tratar, diapositivas, lecturas, etc.\n",
            "* **Preguntar a tu profesor o a un compañero de clase:** La forma más directa de obtener la información es preguntar.\n",
            "* **Revisar tus apuntes de la clase anterior:** A veces el profesor adelanta el temario de la siguiente sesión al final de la clase.\n",
            "\n",
            "\n",
            "¡Espero que esto te ayude!\n",
            "\n",
            "Chatbot: Para resumir un texto, escribe 'resumen' seguido del texto que quieres resumir.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent\"\n",
        "API_KEY = \"AIzaSyAQ2krVgyfmrtQq_FFSubcY-wzf8ZoAyiA\"  # Reemplaza con tu clave API real\n",
        "\n",
        "def enviar_solicitud_gemini(prompt):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"x-goog-api-key\": API_KEY,\n",
        "    }\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": prompt\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "    return response.json()\n",
        "\n",
        "def obtener_respuesta_gemini(respuesta_json):\n",
        "    try:\n",
        "        return respuesta_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except (KeyError, IndexError):\n",
        "        return \"Lo siento, no pude procesar la respuesta.\"\n",
        "\n",
        "print(\"Hola, soy Dudacero.IA ¿En qué puedo ayudarte hoy?\")\n",
        "print(\"1. Resumir texto\")\n",
        "print(\"2. Cronograma y fechas de evaluación\")\n",
        "print(\"3. Temas de la próxima clase\")\n",
        "\n",
        "while True:\n",
        "    usuario_input = input(\"Tú: \")\n",
        "    if usuario_input.lower() in [\"salir\", \"adiós\", \"hasta luego\"]:\n",
        "        print(\"Chatbot: ¡Adiós! Fue un placer hablar contigo.\")\n",
        "        break\n",
        "\n",
        "    if usuario_input == \"1\":\n",
        "        print(\"Chatbot: Para resumir un texto, escribe 'resumen' seguido del texto que quieres resumir.\")\n",
        "        usuario_input = input(\"Tú: \")\n",
        "        prompt = \"Resume el siguiente texto: \" + usuario_input.replace(\"resumen\", \"\")\n",
        "    elif usuario_input == \"2\":\n",
        "        print(\"Chatbot: Para consultar el cronograma o fechas de evaluación, escribe 'fecha límite' seguido de la actividad.\")\n",
        "        usuario_input = input(\"Tú: \")\n",
        "        prompt = f\"¿Cuál es la fecha límite para la entrega de {usuario_input.replace('fecha límite ', '')}?\"\n",
        "    elif usuario_input == \"3\":\n",
        "        prompt = \"¿Qué temas se verán en la próxima clase?\"\n",
        "    else:\n",
        "        prompt = usuario_input\n",
        "\n",
        "    respuesta_json = enviar_solicitud_gemini(prompt)\n",
        "    respuesta_chatbot = obtener_respuesta_gemini(respuesta_json)\n",
        "    print(\"Chatbot:\", respuesta_chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ZH4fUJsGYN",
        "outputId": "fe13675e-b033-403c-a7f0-b71876999b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot-AulaVirtual'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 4.67 MiB | 26.42 MiB/s, done.\n",
            "/content/Chatbot-AulaVirtual\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/serafin99/Chatbot-AulaVirtual.git\n",
        "%cd Chatbot-AulaVirtual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVykRXQYsWsm",
        "outputId": "baa46f47-f36e-4de6-dd66-ef492103e33a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat '/content/dudacero.ia.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv /content/dudacero.ia.ipynb ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1yOLfI0spyu",
        "outputId": "d2bc7af1-6a6e-45b0-9594-e40d54a7b82c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot-AulaVirtual  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IQ1kr_5s3Ot",
        "outputId": "a890e720-0d92-44db-fbea-297e98cd46a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: cd: Chatbot-AulaVirtual: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cd Chatbot-AulaVirtual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwbIMJ2PtByP",
        "outputId": "c276376d-37d7-45d5-b3c5-f31ba22bd5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot-AulaVirtual  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai8NHOt9tLBU",
        "outputId": "a7710265-13c8-476d-8624-93c093b9e175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot-AulaVirtual'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 4.67 MiB | 24.78 MiB/s, done.\n",
            "/content/Chatbot-AulaVirtual/Chatbot-AulaVirtual\n",
            "/bin/bash: line 1: cd: Chatbot-AulaVirtual: No such file or directory\n",
            "'Presentación Prompts inteligencia Artificial Ilustrativo geométrico Verde.pdf'   README.md\n",
            "mv: cannot stat 'dudacero.ia.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/serafin99/Chatbot-AulaVirtual.git\n",
        "%cd Chatbot-AulaVirtual\n",
        "!cd Chatbot-AulaVirtual\n",
        "!ls\n",
        "!mv dudacero.ia.ipynb .  # Asumiendo que el cuaderno se llama así"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj6HRU3btjj6",
        "outputId": "bd4acfcc-b53b-4886-d5b2-d8e8a908a069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot-AulaVirtual'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 4.67 MiB | 25.44 MiB/s, done.\n",
            "/content/Chatbot-AulaVirtual/Chatbot-AulaVirtual/Chatbot-AulaVirtual\n",
            "mv: cannot stat 'dudacero.ia.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/serafin99/Chatbot-AulaVirtual.git\n",
        "%cd Chatbot-AulaVirtual\n",
        "!mv dudacero.ia.ipynb ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_YauqtYuBrQ",
        "outputId": "36856e4a-b41d-4d7e-db64-9a9fcbab5c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot-AulaVirtual  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-9Ayy6gu9iR",
        "outputId": "5304da60-a746-4ec3-cc20-655aef1d9bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Chatbot-AulaVirtual\t\t\t\t\t\t\t\t  README.md\n",
            "'Presentación Prompts inteligencia Artificial Ilustrativo geométrico Verde.pdf'\n"
          ]
        }
      ],
      "source": [
        "!ls /content/Chatbot-AulaVirtual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pb3Akz9vEir",
        "outputId": "d74279b2-5420-4427-ad8e-e0f8dc87136f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'dudacero.ia.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv dudacero.ia.ipynb .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvBTzNyyvL1B",
        "outputId": "f8f438db-4b6c-4eee-edbb-4becbda2dfac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot-AulaVirtual'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 4.67 MiB | 24.03 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/serafin99/Chatbot-AulaVirtual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lc93XzQvQXS",
        "outputId": "6ebf5ac4-d536-447a-bd97-878436d5659c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Chatbot-AulaVirtual/Chatbot-AulaVirtual/Chatbot-AulaVirtual/Chatbot-AulaVirtual\n"
          ]
        }
      ],
      "source": [
        "%cd Chatbot-AulaVirtual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyN8jQGovYi9",
        "outputId": "c55d7cf9-a2c9-48c1-8e4c-66a23b413f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Presentación Prompts inteligencia Artificial Ilustrativo geométrico Verde.pdf'   README.md\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJDVpkxOvaq2",
        "outputId": "92eb7eda-1ead-46a2-fd4f-91a028421fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'dudacero.ia.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv dudacero.ia.ipynb .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAXMWib9uwCR"
      },
      "outputs": [],
      "source": [
        "!find /content/ -name dudacero.ia.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}